{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "xj1F41AShIQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "LFo7Ja6Q-WK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"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\""
      ],
      "metadata": {
        "id": "hABhc9Zer_y6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import ServiceContext, set_global_service_context, VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "def create_index():\n",
        "  documents = SimpleDirectoryReader('./data').load_data()\n",
        "  llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=256)\n",
        "  service_context = ServiceContext.from_defaults(llm=llm)\n",
        "  index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "  index.storage_context.persist()\n",
        "  return index"
      ],
      "metadata": {
        "id": "PFiXOzTAVt1q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import StorageContext, load_index_from_storage\n",
        "\n",
        "def load_index():\n",
        "  storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "  index = load_index_from_storage(storage_context=storage_context)\n",
        "  return index"
      ],
      "metadata": {
        "id": "b3T5jiHYYIXV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import OpenAI\n",
        "from llama_index.memory import ChatMemoryBuffer\n",
        "\n",
        "def create_chat_engine(index, role):\n",
        "  query_engine = index.as_chat_engine()\n",
        "\n",
        "  memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
        "\n",
        "  chat_engine = index.as_chat_engine(\n",
        "      chat_mode=\"context\",\n",
        "      memory=memory,\n",
        "      system_prompt=(\n",
        "          f\"{role[0]}, {role[1]}\"\n",
        "      ),\n",
        "  )\n",
        "  return chat_engine"
      ],
      "metadata": {
        "id": "g6XrQt-FWCPb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roles = {\"Therapist\": [\"Therapist\", \"Empathetic, patient, with strong communication skills to help individuals cope with mental and emotional issues.\"],\n",
        "         \"Psycologist\": [\"Psycologist\", \"Analytical, detail-oriented, with strong research skills to understand human behavior and prpose potential solutions.\"],\n",
        "         \"Coach\": [\"Coach\", \"Motivational, goal-oriented, with strong communication skills to help individuals imptove their personal or profesionnal lives\"]\n",
        "}"
      ],
      "metadata": {
        "id": "65CN5ohj_Tsz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = load_index()\n",
        "chat_engine = create_chat_engine(index, roles[\"Therapist\"])\n",
        "while True:\n",
        "  user_input = input(\"Prompt:\")\n",
        "  if user_input == \"bye\":\n",
        "    break\n",
        "  response = chat_engine.chat(user_input)\n",
        "  print(response.response)\n",
        "chat_engine.reset()"
      ],
      "metadata": {
        "id": "VW3TiVK3dZ6e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}